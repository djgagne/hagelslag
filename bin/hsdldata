#!/home/tmp/aburke/miniconda3/envs/deephail/bin/python
from hagelslag.processing.DLProcessing import DLPreprocessing
from hagelslag.util.Config import Config
from multiprocessing import Pool
import argparse, pdb
from netCDF4 import Dataset
import numpy as np
import os


def main():
    """
    Main function to parse out configuration file (Config), a dictionary 
    of different model tunnings, for slicing model and observational data. 

    For a given number of parallel processesors, the model and observational 
    data are sliced each day with the model data separated by ensemble member. 
    """
    parser = argparse.ArgumentParser("hsdldata - Hagelslag Deep Learning Data Processor")
    parser.add_argument("config", help="Configuration file")
    parser.add_argument("-p", "--proc", type=int, default=1,help="Number of processors")
    parser.add_argument("-o", "--obs", action="store_true", help="Process observed tracks only")
    args = parser.parse_args()
    required = ['dates','start_hour','end_hour','ensemble_members','model_path', 
                'ensemble_name','mrms_path','mrms_variable','storm_variables',
                'potential_variables','model_map_file','hf_path','patch_radius',
                'train','single_step'] 
    #Add attributes of dict to config
    config = Config(args.config, required_attributes=required)
    if not hasattr(config, "run_date_format"):
        config.run_date_format = "%Y%m%d-%H%M"
    if hasattr(config, "mask_file"):
        zero_mask = Dataset(config.mask_file).variables['usa_mask'][:]
        config.mask = np.where(zero_mask.flatten()<1,np.nan,1).reshape(zero_mask.shape)
    else:config.mask=None

    forecast_variables = config.storm_variables + config.potential_variables
    if hasattr(config, "tendency_variables"):forecast_variables.extend(config.tendency_variables)

    #Initiate processing class
    dlprocess = DLPreprocessing(config.ensemble_name,config.model_path,
        config.hf_path,config.patch_radius,config.run_date_format,
        forecast_variables,config.storm_variables, config.potential_variables,
        config.mask)
    #Process map data
    dlprocess.process_map_data(config.model_map_file)
    #Process obs and ensemble data with parallelization
    pool = Pool(args.proc)
    for run_date in config.dates:
        if args.obs or config.train is True:
            pool.apply_async(dlprocess.process_observational_data, 
                (run_date,config.start_hour,config.end_hour,config.mrms_variable,config.mrms_path))
        if config.train is True: 
            for member in config.ensemble_members:
                member_path = '{0}/{1}'.format(config.hf_path,member)
                if not os.path.exists(member_path): os.makedirs(member_path)
                pool.apply_async(dlprocess.process_ensemble_member, 
                    (run_date,member,config.hf_path,config.model_map_file,
                    config.start_hour,config.end_hour,config.single_step))
    pool.close()
    pool.join()
    return

if __name__ == "__main__":
    main()
