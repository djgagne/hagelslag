{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed plots in web page. No floating window.\n",
    "%matplotlib inline\n",
    "# svg increases resolution when you zoom in (Ctrl-+); png does not.\n",
    "# Use svg format (scalable vector graphics) for plots in web page, not png\n",
    "%config InlineBackend.figure_formats=['png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied from https://www.xormedia.com/natural-sort-order-with-zero-padding/\n",
    "\n",
    "from sys import maxint\n",
    "import re\n",
    "\n",
    "# optional '-' to support negative numbers\n",
    "_num_re = re.compile(r'-?\\d+')\n",
    "# number of chars in the largest possible int\n",
    "_maxint_digits = len(str(maxint))\n",
    "# format for zero padding positive integers\n",
    "_zero_pad_int_fmt = '{{0:0{0}d}}'.format(_maxint_digits)\n",
    "# / is 0 - 1, so that negative numbers will come before positive\n",
    "_zero_pad_neg_int_fmt = '/{{0:0{0}d}}'.format(_maxint_digits)\n",
    "\n",
    "\n",
    "def _zero_pad(match):\n",
    "    n = int(match.group(0))\n",
    "    # if n is negative, we'll use the negative format and flip the number using\n",
    "    # maxint so that -2 comes before -1, ...\n",
    "    return _zero_pad_int_fmt.format(n) \\\n",
    "        if n > -1 else _zero_pad_neg_int_fmt.format(n + maxint)\n",
    "\n",
    "def zero_pad_numbers(s):\n",
    "    return _num_re.sub(_zero_pad, s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import pandas as pd\n",
    "\n",
    "def spc_hailwind_filename(hailwind,year):\n",
    "    name = '/glade/u/home/ahijevyc/share/'+hailwind+'/'\n",
    "    if hailwind=='torn':\n",
    "        return name + 'Actual_tornadoes.csv'\n",
    "    if year>=2008:\n",
    "        name = name + str(year) + \"_\" + hailwind + \".csv\"\n",
    "    if year >= 2005 and year <= 2007:\n",
    "        name = name + \"2005-2007_\"+hailwind+\".csv\"\n",
    "    return name\n",
    "\n",
    "\n",
    "def filter_rpts(rpts, start, end):\n",
    "    # convert GMT to CST\n",
    "    if any(rpts['tz'] == 9):\n",
    "        rpts['yr_mo_dy_time'][rpts['tz']==9] = rpts['yr_mo_dy_time'] - dt.timedelta(hours=6)\n",
    "        rpts['tz'][rpts['tz']==9] = 3\n",
    "        \n",
    "    if any(rpts['tz'] != 3):\n",
    "        print start, end\n",
    "        print rpts[['om','yr_mo_dy_time','tz']][rpts['tz'] != 3]\n",
    "        print \"WARNING - proceeding with program. Wrote to SPC Mar 22 2017 about fixing these lines\"\n",
    "    \n",
    "    times = rpts['yr_mo_dy_time'] + dt.timedelta(hours=6) # Convert from CST to UTC\n",
    "    correct_date = (times >= start) & (times < end)\n",
    "    return rpts[correct_date]\n",
    "\n",
    "\n",
    "def storm_rpts(type, start, end):\n",
    "    \n",
    "    possible_types = [\"wind\", \"hail\", \"torn\"]\n",
    "    if type not in possible_types:\n",
    "        print \"unknown storm report type:\", type\n",
    "        print \"must be \", possible_types\n",
    "        sys.exit(2)\n",
    "\n",
    "    # csv format described in http://www.spc.noaa.gov/wcm/data/SPC_severe_database_description.pdf\n",
    "    # SPC storm report files downloaded from http://www.spc.noaa.gov/wcm/#data to \n",
    "    # yellowstone: ~ahijevyc/share/ March 2017.\n",
    "    if type == \"wind\":\n",
    "        cols = ['om','yr','mo','dy','date','time','tz', 'st','stf','stn','mag','inj',\n",
    "                'fat','loss','closs','slat','slon','elat','elon','len','wid','ns',\n",
    "                'sn','sg','f1','f2','f3','f4','mt']\n",
    "    if type == \"hail\":\n",
    "        cols = ['om','yr','mo','dy','date','time','tz', 'st','stf','stn','sz','inj',\n",
    "                'fat','loss','closs','slat','slon','elat','elon','len','wid','ns',\n",
    "                'sn','sg','f1','f2','f3','f4','mt']\n",
    "    if type == \"torn\":\n",
    "        cols = ['om','yr','mo','dy','date','time','tz', 'st','stf','stn','f','inj',\n",
    "                'fat','loss','closs','slat','slon','elat','elon','len','wid','ns',\n",
    "                'sn','sg','f1','f2','f3','f4','mt']\n",
    "\n",
    "    rpts_file = spc_hailwind_filename(type,run_date.year)\n",
    "    rpts = pd.read_csv(rpts_file,header=None,names=cols, parse_dates=[['yr','mo','dy','time']],\n",
    "                       infer_datetime_format=True)\n",
    "    \n",
    "    rpts = filter_rpts(rpts, start, end)\n",
    "    return rpts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon, PathPatch, Patch\n",
    "from matplotlib.collections import PatchCollection\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from glob import glob\n",
    "from itertools import cycle\n",
    "import sys, json, os\n",
    "import numpy as np\n",
    "from hagelslag.data import ModelOutput\n",
    "from hagelslag.processing import read_geojson\n",
    "from hagelslag.util.make_proj_grids import read_ncar_map_file, make_proj_grids\n",
    "from netCDF4 import Dataset\n",
    "from mysavfig import mysavfig\n",
    "import numbers\n",
    "plt.rcParams.update({'mathtext.default': 'regular'})\n",
    "\n",
    "def load_json_tracks(path, run_date, member, model_format):\n",
    "    storm_tracks = []\n",
    "    search_str = path + run_date.strftime(\"%Y%m%d/\") + member + \"/\" + model_format + \"*.json\"\n",
    "\n",
    "    model_files = sorted(glob(search_str), key=zero_pad_numbers)\n",
    "    if not model_files:\n",
    "        print \"in load_json_tracks(), no model_files found.\"\n",
    "        print \"run_date=\",run_date,\"member=\",member,\"model_format=\",model_format\n",
    "        print \"search str=\",search_str\n",
    "\n",
    "\n",
    "    old_mtime = 0\n",
    "    for model_file in model_files:\n",
    "        # Check if modification time is earlier than previous file. \n",
    "        # This indicates an older run still cluttering the directory.\n",
    "        mtime = os.path.getmtime(model_file)\n",
    "        if mtime < old_mtime:\n",
    "            print model_file, mtime, \"was modified before\", old_model_file, old_mtime\n",
    "            print \"an older run may be cluttering \"+search_str\n",
    "            print \"removing\", model_file\n",
    "            os.remove(model_file)\n",
    "            continue\n",
    "        storm_tracks.append(read_geojson(model_file))\n",
    "        old_mtime = mtime\n",
    "        old_model_file = model_file\n",
    "    return storm_tracks\n",
    "\n",
    "def add_basemap(ax,drawcounties=True):\n",
    "    proj_dict, grid_dict = read_ncar_map_file(\"/glade/p/work/ahijevyc/hagelslag/mapfiles/VSE.txt\")\n",
    "    m = Basemap(resolution=\"i\", # \"c\"< \"l\"< \"i\" <\"h\"\n",
    "            llcrnrlon=grid_dict[\"sw_lon\"],\n",
    "            urcrnrlon=grid_dict[\"ne_lon\"],\n",
    "            llcrnrlat=grid_dict[\"sw_lat\"],\n",
    "            urcrnrlat=grid_dict[\"ne_lat\"],\n",
    "            rsphere = (proj_dict[\"a\"], proj_dict[\"b\"]), \n",
    "            projection=proj_dict[\"proj\"], lat_2=proj_dict[\"lat_2\"], lat_1=proj_dict[\"lat_1\"],\n",
    "             lat_0=proj_dict[\"lat_0\"], lon_0=proj_dict[\"lon_0\"], ax=ax)\n",
    "    if drawcounties:\n",
    "        m.drawcounties(linewidth=0.05)\n",
    "    m.drawstates()\n",
    "    m.drawcountries()\n",
    "    m.drawcoastlines(linewidth=0.5)\n",
    "    m.drawparallels(np.arange(0.,81.,2.),labels=[True,False,False,False],linewidth=0.4)\n",
    "    meridians = np.arange(0.,351.,2.)\n",
    "    m.drawmeridians(meridians,labels=[False,False,False,True],linewidth=0.4)\n",
    "    return m\n",
    "\n",
    "\n",
    "def output_netcdf_file(filename, mask_grid, proj_dict, grid_dict):\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "        print \"removed old\",filename\n",
    "    basedir = os.path.dirname(filename)\n",
    "    if not os.path.exists(basedir):\n",
    "        print \"trying to write\",os.path.basename(filename),\"but directory\",basedir,\"must be created first\"\n",
    "        os.makedirs(basedir)\n",
    "    try:\n",
    "        out_set = Dataset(filename, \"w\", clobber=False) # Set clobber to True only if you want to recreate terrain-anchored object file.\n",
    "    except:\n",
    "        print \"not creating\", filename\n",
    "        return\n",
    "    print \"creating\", filename\n",
    "    dx = grid_dict['dx']\n",
    "    out_set.createDimension(\"y\", mask_grid.shape[0])\n",
    "    out_set.createDimension(\"x\", mask_grid.shape[1])\n",
    "    out_set.set_auto_mask(True)\n",
    "    var = out_set.createVariable(\"count\", 'u8', (\"y\", \"x\"), zlib=True)\n",
    "    var[:] = mask_grid\n",
    "    var.long_name = \"object count\"\n",
    "    x = out_set.createVariable(\"x\", 'double', (\"x\"))\n",
    "    x[:] = dx * np.arange(mask_grid.shape[1])\n",
    "    x.units = \"m\"\n",
    "    x.long_name  = \"x coordinate of projection\"\n",
    "    y = out_set.createVariable(\"y\", 'double', (\"y\"))\n",
    "    y[:] = dx * np.arange(mask_grid.shape[0])\n",
    "    y.units = \"m\"\n",
    "    y.long_name  = \"y coordinate of projection\"\n",
    "\n",
    "    latitude = out_set.createVariable(\"latitude\", 'double', (\"y\",\"x\"), zlib=True)\n",
    "    latitude[:] = grid_dict[\"lat\"]\n",
    "    latitude.units = \"degrees_north\"\n",
    "    latitude.long_name = \"latitude\"\n",
    "    longitude = out_set.createVariable(\"longitude\", 'double', (\"y\",\"x\"), zlib=True)\n",
    "    longitude[:] = grid_dict[\"lon\"]\n",
    "    longitude.units = \"degrees_east\"\n",
    "    longitude.long_name = \"longitude\"\n",
    "    \n",
    "    \n",
    "    for k, v in proj_dict.items():\n",
    "        setattr(out_set, k, v)\n",
    "    for k, v in grid_dict.items():\n",
    "        if k in [\"i\", \"j\", \"lon\", \"lat\", \"x\", \"y\"]:\n",
    "            #print \"skipping\",k,v\n",
    "            continue\n",
    "        try:\n",
    "            #print 'setting',k,v\n",
    "            setattr(out_set, k, v)\n",
    "        except:\n",
    "            pass\n",
    "    out_set.close()\n",
    "    print \"created\",filename\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "members = [\"3km_pbl1\",\"1km_on_3km_pbl1\"]\n",
    "#members = [\"1km_on_3km_pbl2\"]\n",
    "model_format = \"VSE\"\n",
    "ensemble_name = \"VSE\"\n",
    "\n",
    "ncf = Dataset(\"/glade/p/work/ahijevyc/hagelslag/mapfiles/\"+model_format+\"_mask.nc\")\n",
    "landmask = ncf.variables[\"usa_mask\"][:]\n",
    "ncf.close()\n",
    "#ncea -y ttl /glade/scratch/ahijevyc/OBJECT_TRACK/track_data_VSE_json_WSPD10MAX_15/2*/*3km_pbl1/*.object_count.nc terrain_obj_file\n",
    "terrain_obj_file = \"/glade/p/work/ahijevyc/hagelslag/out/u.nc\"\n",
    "ncf = Dataset(terrain_obj_file)\n",
    "nobj = ncf.variables[\"count\"][:]\n",
    "ncf.close()\n",
    "\n",
    "model_path = \"/glade/scratch/ahijevyc/\"+ensemble_name+\"/\"\n",
    "odir = \"/glade/p/work/ahijevyc/hagelslag/out/\"\n",
    "#fields = [\"MAX_UPDRAFT_HELICITY_25\"] * len(members)\n",
    "fields = [\"MAX_UPDRAFT_HELICITY_87\"]\n",
    "#fields = [\"UP_HELI_MAX03_22\"]#,\"UP_HELI_MAX03_77\"]\n",
    "#fields = [\"UP_HELI_MAX03_25\"] * 2\n",
    "#fields = [\"WSPD10MAX_15\"] * 2\n",
    "#fields = \"HAIL2D\"\n",
    "#fields = [\"REFL_1KM_AGL_40\"] * 2\n",
    "if len(members) != len(fields):\n",
    "    print \"# members not equal to # fields\", members, fields\n",
    "    sys.exit(2)\n",
    "pixel_thresh = 1\n",
    "SPC_rpts = False\n",
    "\n",
    "dates = [datetime(2005, 1,13,12,tzinfo=pytz.UTC),\n",
    "         datetime(2005,12,28,12,tzinfo=pytz.UTC),\n",
    "         datetime(2006, 1,13,12,tzinfo=pytz.UTC),\n",
    "         datetime(2007, 1, 4,12,tzinfo=pytz.UTC),\n",
    "         datetime(2007, 1, 7,12,tzinfo=pytz.UTC),\n",
    "         datetime(2007, 2,12,12,tzinfo=pytz.UTC),\n",
    "         datetime(2007, 2,13,12,tzinfo=pytz.UTC),\n",
    "         datetime(2007, 2,24,12,tzinfo=pytz.UTC),\n",
    "         datetime(2008, 2,12,12,tzinfo=pytz.UTC),\n",
    "         datetime(2008, 2,16,12,tzinfo=pytz.UTC),\n",
    "         datetime(2008, 2,17,12,tzinfo=pytz.UTC),\n",
    "         datetime(2008, 2,25,12,tzinfo=pytz.UTC),\n",
    "         datetime(2008,12, 9,12,tzinfo=pytz.UTC),\n",
    "         datetime(2009, 2,18,12,tzinfo=pytz.UTC),\n",
    "         datetime(2009,12,24,12,tzinfo=pytz.UTC),\n",
    "         datetime(2010, 1,20,12,tzinfo=pytz.UTC),\n",
    "         datetime(2010,12,31,12,tzinfo=pytz.UTC),\n",
    "         datetime(2011, 2,28,12,tzinfo=pytz.UTC),\n",
    "         datetime(2011,12,22,12,tzinfo=pytz.UTC),\n",
    "         datetime(2012, 1,22,12,tzinfo=pytz.UTC),\n",
    "         datetime(2012, 1,25,12,tzinfo=pytz.UTC)]\n",
    "\n",
    "#dates = [ datetime(2008,2,25,12,tzinfo=pytz.UTC) ]\n",
    "for run_date in dates:\n",
    "\n",
    "    rows = len(members)+1\n",
    "    fig, ax = plt.subplots(rows, figsize=(8.5,5*rows))\n",
    "    color_list = cycle([\"violet\", \"green\", \"cyan\", \"blue\", \"purple\", \"darkgreen\", \"teal\", \"royalblue\"])\n",
    "\n",
    "    windrpts = storm_rpts(\"wind\", run_date, run_date + dt.timedelta(hours=24))\n",
    "    hailrpts = storm_rpts(\"hail\", run_date, run_date + dt.timedelta(hours=24))\n",
    "    tornrpts = storm_rpts(\"torn\", run_date, run_date + dt.timedelta(hours=24))\n",
    "\n",
    "    wlons, wlats = windrpts['slon'].values, windrpts['slat'].values\n",
    "    hlons, hlats = hailrpts['slon'].values, hailrpts['slat'].values\n",
    "    tlons, tlats = tornrpts['slon'].values, tornrpts['slat'].values\n",
    "\n",
    "    m = add_basemap(ax[-1])\n",
    "    final_legend_list = []\n",
    "    iax = -1\n",
    "    \n",
    "    for member, field in zip(members,fields):\n",
    "        json_path = \"/glade/scratch/ahijevyc/OBJECT_TRACK/track_data_\"+ensemble_name+\"_json_\"+field+\"/\"\n",
    "\n",
    "        terraintoo, watertoo = True, True # plot objects over water too\n",
    "        if \"WSPD10MAX\" in field:\n",
    "            watertoo = False # don't plot wind objects over water.\n",
    "            terraintoo = False # don't plot wind objects anchored to terrain.\n",
    "\n",
    "        iax = iax+1\n",
    "        m = add_basemap(ax[iax])\n",
    "\n",
    "        centroids = []\n",
    "        object_sizes = []\n",
    "        model_grid = ModelOutput(ensemble_name,\n",
    "                                 member, run_date, field, run_date, \n",
    "                                 run_date+timedelta(hours=24),\n",
    "                                 model_path,\n",
    "                                 single_step=True)\n",
    "\n",
    "        model_map_file=\"/glade/p/work/ahijevyc/hagelslag/mapfiles/VSE.txt\"\n",
    "        model_grid.load_map_info(model_map_file)\n",
    "        model_grid.data = []\n",
    "\n",
    "        # Initialze overlay_grid with zeros - counts number of times a points is occupied by an object\n",
    "        # over the couse of a model run. One netCDF file is produced for each date and each member.\n",
    "        # Later the output netCDF files can be merged to identify points with objects most \n",
    "        # frequently.  These are likely objects anchored to terrain.\n",
    "        nobj_grid = np.zeros(model_grid.lat.shape, dtype=int)\n",
    "        nobj_nc = json_path + run_date.strftime(\"%Y%m%d/\") + member + run_date.strftime('/%Y%m%d%H.')+\"object_count.nc\"\n",
    "\n",
    "        storm_tracks = load_json_tracks(json_path, run_date, member, model_format)\n",
    "        if not storm_tracks:\n",
    "            # Used to indicate a bug, but this happens legitimately if no objects are found (e.g. 20080225 no 0-3km UH>25 m2/s2)\n",
    "            pass\n",
    "        patches = []\n",
    "        for track in storm_tracks:\n",
    "            previous_centroid = None\n",
    "            for time, mask, i, j in zip(track.times, track.masks, track.i, track.j):\n",
    "                osize = track.size(time)\n",
    "                if osize < pixel_thresh:\n",
    "                    print 'skipping small object, %d pixels'% osize\n",
    "                    continue\n",
    "\n",
    "                polygon_xy = track.boundary_polygon(time)\n",
    "                \n",
    "                # Get ij indices covered by object and increment nobj_grid by 1.\n",
    "                ni = (mask * i)[np.nonzero(mask)]\n",
    "                nj = (mask * j)[np.nonzero(mask)]\n",
    "                nobj_grid[ni,nj] = nobj_grid[ni,nj] + 1\n",
    "\n",
    "                if not watertoo and np.mean(landmask[ni,nj]) < 0.5:\n",
    "                    tmp = track.center_of_mass(time)\n",
    "                    print \"object >50% over water. ignoring\", model_grid.proj(*tmp, inverse=True)\n",
    "                    continue\n",
    "                average_obj_frequency = np.mean(nobj[ni,nj])\n",
    "                if not terraintoo and average_obj_frequency > 50:\n",
    "                    tmp = track.center_of_mass(time)\n",
    "                    print \"average obj frequency\", average_obj_frequency\n",
    "                    print \"may be anchored to terrain. ignoring\", model_grid.proj(*tmp, inverse=True)\n",
    "                    continue\n",
    "                    \n",
    "                object_sizes.append(osize)\n",
    "                centroid = track.center_of_mass(time)\n",
    "                # convert centroid from xkm ykm to lon lat\n",
    "                centroid = model_grid.proj(*centroid, inverse=True)\n",
    "                centroids.append(centroid)\n",
    "\n",
    "                # expand polygon_lonlat into two arguments (asterisk)\n",
    "                # Feed to m() map transformation\n",
    "                # Transpose 2xN array to Nx2. \n",
    "                # Sometimes boundary_polygon returns 2 empty arrays (4 pixels?). Don't plot it if that is the case\n",
    "                if polygon_xy[0].size > 0:\n",
    "                    # use these values in xkm and ykm.\n",
    "                    # Use model_grid.proj object to go to lonlat\n",
    "                    polygon_lonlat = model_grid.proj(*polygon_xy, inverse=True)\n",
    "                    patches.append(Polygon(np.transpose(m(*polygon_lonlat)), closed=True, fill=True))\n",
    "                else:\n",
    "                    print \"no boundary polygon. size\", track.size(time), \"mask\", mask\n",
    "\n",
    "                # label storm objects with hour\n",
    "                ax[iax].annotate(str(time), xy=m(*centroid), fontsize=5.1)\n",
    "                # Draw line from previous object to this one.\n",
    "                if previous_centroid is not None:\n",
    "                    m.drawgreatcircle(previous_centroid[0], previous_centroid[1], centroid[0], centroid[1],color=\"black\")\n",
    "                previous_centroid = centroid\n",
    "\n",
    "        output_netcdf_file(nobj_nc,nobj_grid,{\"pixel_thresh\":pixel_thresh},model_grid.__dict__)\n",
    "        \n",
    "        area = np.sum(object_sizes)*(model_grid.dx/1000)**2\n",
    "        color = next(color_list)\n",
    "        ax[iax].set_title(member + \" \" + run_date.strftime(\"%Y%m%d%H\")+ \" \" + str(area) + \" km\" + r'$^2$' +\"\\n\"+\n",
    "                          field+ r'$ \\geq$' +\"%d pixels\"%pixel_thresh + \" watertoo: \" + str(watertoo) +\n",
    "                          \" terraintoo: \" + str(terraintoo))\n",
    "        alpha = 0.38\n",
    "        pc = PatchCollection(patches, color=color, alpha=alpha)\n",
    "        ax[iax].add_collection(pc) # Plot individual member\n",
    "\n",
    "        ax[iax].legend(handles=[Patch(color=color,alpha=alpha,label=member)],loc=\"upper left\",numpoints=1,fontsize=9)\n",
    "        # For some reason, if I substitute the literal PatchCollection instance below with \n",
    "        # the variable \"pc\", no patches appear in any of the subplots.\n",
    "        # Tack on individual member to final, composite plot.\n",
    "        # First member is opaque; successive members are more transparent\n",
    "        alpha = 1-np.sqrt(float(iax)/len(members))\n",
    "        ax[-1].add_collection(PatchCollection(patches, color=color, alpha=alpha))\n",
    "        final_legend_list.append(Patch(color=color,alpha=alpha,label=member))\n",
    "\n",
    "        if len(centroids) > 0:\n",
    "            # Get area-weighted centroid for all objects\n",
    "            overall_centroid = np.average(centroids, weights = object_sizes, axis=0)\n",
    "            m.plot(*overall_centroid, marker='x', color=color, markersize=40, latlon=True)\n",
    "            centroid_on_final_panel, = m.plot(*overall_centroid, marker='x', color=color, \n",
    "                                              label = member + ' centroid', markersize=40, latlon=True, \n",
    "                                              linestyle='None', ax=ax[-1])\n",
    "            final_legend_list.append(centroid_on_final_panel)\n",
    "            \n",
    "    \n",
    "    ax[-1].set_title(','.join(members)+ \" \" + run_date.strftime(\"%Y%m%d%H\")+\"\\n\"+'_'.join(fields)+ r'$ \\geq$'+ \"%d pixels\"%pixel_thresh)\n",
    "\n",
    "    norpts_str = \".norpts\"\n",
    "    if SPC_rpts:\n",
    "        wrpts, = m.plot(wlons, wlats, 'bo', markersize=2.5, marker=\"s\", latlon=True, label=\"Wind Report\", alpha=0.5, ax=ax[-1])\n",
    "        hrpts, = m.plot(hlons, hlats, 'go', markersize=2.8, marker=\"^\", latlon=True, label=\"Hail Report\", alpha=0.5, ax=ax[-1])\n",
    "        trpts, = m.plot(tlons, tlats, 'ro', markersize=3.0, marker=\"v\", latlon=True, label=\"Tornado Rpt\", alpha=0.5, ax=ax[-1])\n",
    "        final_legend_list.extend([wrpts, hrpts, trpts])\n",
    "        norpts_str = \"\"\n",
    "\n",
    "\n",
    "    ax[-1].legend(handles=final_legend_list,loc=\"upper left\",numpoints=1,fontsize=7.,framealpha=0.5)\n",
    "    ofile = odir + '_'.join(members) + \"_\" + run_date.strftime(\"%Y%m%d%H_\") + '_'.join(fields) + norpts_str + \".png\" # multi-row plot: 1 per member, plus 1 all-member composite.\n",
    "    ret = mysavfig(ofile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the number of objects on each grid point. Assumes this has already been made with ncea. \n",
    "# Reads 'history' attribute to get number of files. Assumes each file is sum of 24 times.\n",
    "\n",
    "\n",
    "basedir = \"/glade/scratch/ahijevyc/OBJECT_TRACK/\"\n",
    "obj_count_file = basedir + \"v.nc\"\n",
    "obj_count_file = basedir + \"track_data_VSE_json_MAX_UPDRAFT_HELICITY_87/20070224/1km_on_3km_pbl1/2007022412.object_count.nc\"\n",
    "obj_count_file = basedir + \"track_data_VSE_json_UP_HELI_MAX03_77/20070224/1km_on_3km_pbl1/2007022412.object_count.nc\"\n",
    "\n",
    "ncf = Dataset(obj_count_file)\n",
    "nobj = ncf.variables[\"count\"][:]\n",
    "if 'history' in ncf.ncattrs():\n",
    "    history = ncf.getncattr('history')\n",
    "    ncf.close()\n",
    "    print \"reading global history attribute\"\n",
    "\n",
    "\n",
    "    # Assumes global 'history' attribute is written by NCO tool.\n",
    "    #// global attributes:\n",
    "    #\t\t:history = \"Mon Mar 27 15:53:03 2017: ncea -y ttl track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20050113/3km_pbl7/2005011312.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20051228/3km_pbl7/2005122812.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20060113/3km_pbl7/2006011312.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20070104/3km_pbl7/2007010412.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20070107/3km_pbl7/2007010712.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20070212/3km_pbl7/2007021212.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20070213/3km_pbl7/2007021312.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20070224/3km_pbl7/2007022412.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20080212/3km_pbl7/2008021212.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20080216/3km_pbl7/2008021612.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20080217/3km_pbl7/2008021712.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20080225/3km_pbl7/2008022512.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20081209/3km_pbl7/2008120912.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20090218/3km_pbl7/2009021812.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20091224/3km_pbl7/2009122412.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20100120/3km_pbl7/2010012012.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20101231/3km_pbl7/2010123112.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20110228/3km_pbl7/2011022812.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20111222/3km_pbl7/2011122212.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20120122/3km_pbl7/2012012212.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_25/20120125/3km_pbl7/2012012512.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_87/20070107/1km_on_3km_pbl7/2007010712.object_count.nc track_data_VSE_json_MAX_UPDRAFT_HELICITY_87/20090218/1km_on_3km_pbl7/2009021812.object_count.nc t.nc\" ;\n",
    "    # Created by ncea -y ttl track_data_VSE_json_UP_HELI_MAX03_??/20*/*3km_pbl1/20*12.object_count.nc t.nc\n",
    "    # in /glade/scratch/ahijevyc/OBJECT_TRACK\n",
    "\n",
    "    dates = [x for x in history.split(' ') if '.object_count.nc' in x]\n",
    "    vmax = 2.\n",
    "else:\n",
    "    # start with \"track_data_\" part of file path\n",
    "    dates = [obj_count_file[obj_count_file.index('track_data_'):]]\n",
    "    vmax = 20.\n",
    "\n",
    "print dates\n",
    "ntimes = len(dates) * 24\n",
    "print \"got\",ntimes,\"times\"\n",
    "\n",
    "fields = [x.split('/')[0].split('json_')[-1] for x in dates]\n",
    "members = [ x.split('/')[2] for x in dates]\n",
    "field_members = [x + ' ' + y for x,y in zip(fields,members)]\n",
    "\n",
    "field_members = set(field_members) # set will ensure unique members\n",
    "fields = set(fields)# set will ensure unique members\n",
    "print \"fields=\",fields\n",
    "members = set(members)# set will ensure unique members\n",
    "print \"members=\",members\n",
    "print \"got\", field_members\n",
    "\n",
    "if 'pixel_thresh' in ncf.ncattrs():\n",
    "    pixel_thresh = ncf.getncattr('pixel_thresh')\n",
    "    print \"got pixel_thresh=\",pixel_thresh,\"from netcdf file\"\n",
    "else:\n",
    "    pixel_thresh = 1\n",
    "    print \"assuming pixel_thresh=1\"\n",
    "    \n",
    "    \n",
    "    \n",
    "print np.max(nobj)    \n",
    "    \n",
    "# Sanity check. Make sure all model_grids are the same.\n",
    "run_date = dt.datetime(2011,2,28,12) # arbitrary date\n",
    "model_grids = []\n",
    "for field, member in zip(fields,members):\n",
    "    model_grids.append(ModelOutput('VSE', member, run_date, field, run_date,\n",
    "                                    run_date+timedelta(hours=24),\"/glade/scratch/ahijevyc/VSE/\",\n",
    "                                    single_step=True))\n",
    "for model_grid in model_grids:\n",
    "    if model_grid.proj != model_grids[0].proj:\n",
    "        print model_grids[0], model_grid\n",
    "        print model_grids[0].proj, model_grid.proj\n",
    "        sys.exit(1)\n",
    "    \n",
    "model_map_file=\"/glade/p/work/ahijevyc/hagelslag/mapfiles/VSE.txt\"\n",
    "model_grid.load_map_info(model_map_file)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.5,5))\n",
    "m = add_basemap(ax)\n",
    "# convert to number of objects to percentage, mask pixels with no objects\n",
    "image = np.ma.array(nobj/float(ntimes)*100.,mask=nobj<1)\n",
    "print np.max(image)\n",
    "# sample the colormaps that you want to use. Use 128 from each so we get 256\n",
    "# colors in total\n",
    "colors1 = plt.cm.Blues(np.linspace(0., 1, 128))\n",
    "colors2 = plt.cm.Greens_r(np.linspace(0, 1, 128))\n",
    "\n",
    "# combine them and build a new colormap\n",
    "colors = np.vstack((colors1, colors2))\n",
    "import matplotlib.colors as mcolors\n",
    "mymap = mcolors.LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "\n",
    "cax = m.contourf(model_grid.lon,model_grid.lat, image, cmap=mymap,latlon=True,vmin=0., vmax=vmax)\n",
    "ax.set_title('\\n'.join(field_members) + \"\\nobjects \" + r'$\\geq$' + \" %d pixels\"%pixel_thresh + \" %d times\"%ntimes)\n",
    "ax.set_ylim=(0,2)\n",
    "cbar = fig.colorbar(cax,label=\"%\")\n",
    "\n",
    "mysavfig(obj_count_file + \".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf = Dataset(basedir + \"u.nc\")\n",
    "print ncf.variables\n",
    "nobj = ncf.variables[\"count\"][:]\n",
    "print nobj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(plt.cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
